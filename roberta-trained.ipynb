{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Downloads","metadata":{"id":"eApES3bhjelX"}},{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"id":"aV33rYn9Vywp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q condacolab\nimport condacolab\ncondacolab.install()","metadata":{"id":"z0uQDvlQ-flz","outputId":"b64bc43c-d355-490f-e3a1-373af2ae96a3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tsnecuda","metadata":{"id":"crwHEI-B-eDo","outputId":"ba12fefd-1ba7-432a-ab5e-a527ec178d0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda install --offline tsnecuda-2.1.0-cuda101.tar.bz2","metadata":{"id":"XpxDI6Lv-1xl","outputId":"b0157ca1-9d92-4413-a5e4-ce7b2fe4a8ff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://anaconda.org/CannyLab/tsnecuda/2.1.0/download/linux-64/tsnecuda-2.1.0-cuda101.tar.bz2\n!tar xvjf tsnecuda-2.1.0-cuda101.tar.bz2\n!cp -r site-packages/* /usr/local/lib/python3.7/dist-packages/","metadata":{"id":"icQVY6VZ-iTv","outputId":"8c424d47-7837-476c-cb2e-7d5dd0b5fa16"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"id":"GTTlsx7xDNzp","outputId":"044a660e-c450-4caa-b153-f6d89109fdcd","execution":{"iopub.status.busy":"2021-12-18T11:49:32.047011Z","iopub.execute_input":"2021-12-18T11:49:32.047371Z","iopub.status.idle":"2021-12-18T11:49:41.035760Z","shell.execute_reply.started":"2021-12-18T11:49:32.047289Z","shell.execute_reply":"2021-12-18T11:49:41.034919Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install datasets","metadata":{"id":"lT_xBtBAl9uW","outputId":"34b1ed74-f171-46ca-e909-4b062efd993e","execution":{"iopub.status.busy":"2021-12-18T11:49:41.038184Z","iopub.execute_input":"2021-12-18T11:49:41.038641Z","iopub.status.idle":"2021-12-18T11:49:48.217513Z","shell.execute_reply.started":"2021-12-18T11:49:41.038602Z","shell.execute_reply":"2021-12-18T11:49:48.216690Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install livelossplot","metadata":{"id":"7XmMfofTnyHN","outputId":"274c7f92-32e8-484a-8594-b99dd491400f","execution":{"iopub.status.busy":"2021-12-18T11:49:48.218874Z","iopub.execute_input":"2021-12-18T11:49:48.223819Z","iopub.status.idle":"2021-12-18T11:49:55.673711Z","shell.execute_reply.started":"2021-12-18T11:49:48.223776Z","shell.execute_reply":"2021-12-18T11:49:55.672753Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{"id":"XBpBYp5Ljs3P"}},{"cell_type":"code","source":"import torch\nfrom torch.optim import Adam\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom transformers import RobertaTokenizer, RobertaConfig, RobertaModel\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm, trange\nimport transformers\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom livelossplot import PlotLosses\nfrom torch.utils import data \nimport datetime\n\n\n#import tsnecuda\n#from tsnecuda import TSNE as TSNE_CUDA\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport gc\n\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","metadata":{"id":"QWtTdMEdnxel","execution":{"iopub.status.busy":"2021-12-18T11:49:55.678115Z","iopub.execute_input":"2021-12-18T11:49:55.678443Z","iopub.status.idle":"2021-12-18T11:50:02.438169Z","shell.execute_reply.started":"2021-12-18T11:49:55.678408Z","shell.execute_reply":"2021-12-18T11:50:02.437443Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data\n\n#### Dataset Description\n\n\n\n*   The dataset comprises of 12K samples\n*   The claims have been classfied as follows\n\n  *   Falset\n  *   Mixture \n  *   True\n  *   Unproven\n\n\n\n\n\n\n\n","metadata":{"id":"YMUzw78XjiXU"}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"health_fact\")","metadata":{"id":"9N9FuRc-mDnr","outputId":"32920f0f-bd82-47bc-9586-3b405dd9e556","execution":{"iopub.status.busy":"2021-12-18T11:50:02.439359Z","iopub.execute_input":"2021-12-18T11:50:02.439622Z","iopub.status.idle":"2021-12-18T11:50:14.864235Z","shell.execute_reply.started":"2021-12-18T11:50:02.439588Z","shell.execute_reply":"2021-12-18T11:50:14.863518Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#dataset","metadata":{"id":"j81Y9isNoI7D","outputId":"fd223fa7-e7b8-4cb3-d230-f0703a1a59cd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset['train'][0]","metadata":{"id":"Gr51gQ-6qNl9","outputId":"ea5209df-53e9-4592-f995-a07db181ba65"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''labels = dataset['train'].features['label'].names\nnum_classes = dataset['train'].features['label'].num_classes'''","metadata":{"id":"jWsWq5kcmUpP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{"id":"_eAwiFro_7fR","outputId":"9a19be74-cfbc-4a5d-dec5-2799498d7910"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#filtering out samples which have -1 as their label\ndataset = dataset.filter(lambda x: x['label'] != -1)","metadata":{"id":"X8cB14o9kOW_","outputId":"65d22ec7-f32f-4b2a-990e-6aec1ef03627","execution":{"iopub.status.busy":"2021-12-18T11:50:14.866087Z","iopub.execute_input":"2021-12-18T11:50:14.866539Z","iopub.status.idle":"2021-12-18T11:50:15.345715Z","shell.execute_reply.started":"2021-12-18T11:50:14.866492Z","shell.execute_reply":"2021-12-18T11:50:15.344863Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Part A \n# Using the RoBERTa model for text classification\n\n\n*  In this part, I will be using the sequence classification model from hugging face for RoBERTa to classify the medical claims\n*   The model will be finetuned on the training dataset comprising of 9k samples\n\n","metadata":{"id":"Bhsv4Jvgzd6S"}},{"cell_type":"markdown","source":"## Loading Model\n\n\n1.   The model used here is the RoBERTa\n2.   This model has shown significant imprvements over the base BERT model, which is supported by the better perfromance of the model.\n\n\n","metadata":{"id":"ucIVtlRwkueW"}},{"cell_type":"code","source":"bert_version = 'roberta-base'\ntokenizer = RobertaTokenizer.from_pretrained(bert_version)","metadata":{"id":"U9jiOKPgoOZk","execution":{"iopub.status.busy":"2021-12-18T11:50:19.686589Z","iopub.execute_input":"2021-12-18T11:50:19.687155Z","iopub.status.idle":"2021-12-18T11:50:21.476045Z","shell.execute_reply.started":"2021-12-18T11:50:19.687117Z","shell.execute_reply":"2021-12-18T11:50:21.475263Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#tokenization\n\ndef encode(example):\n    encodings = tokenizer(example['main_text'], truncation=True, padding='max_length')\n    return { **encodings, 'labels':example['label'] }\n\n\ntokenized_dataset = dataset.map(encode)\ntokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask' ,'labels'])","metadata":{"id":"QB-rLXgioS5j","outputId":"ec609c78-11f2-40f3-8d15-28ea708e0c67","execution":{"iopub.status.busy":"2021-12-18T11:50:23.942831Z","iopub.execute_input":"2021-12-18T11:50:23.943395Z","iopub.status.idle":"2021-12-18T11:52:11.892591Z","shell.execute_reply.started":"2021-12-18T11:50:23.943355Z","shell.execute_reply":"2021-12-18T11:52:11.891921Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset.remove_columns(['claim_id', 'label', 'claim','date_published','explanation','fact_checkers','sources','subjects','main_text'])","metadata":{"id":"73q7fm6IKIhW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = dict()\ndata['train'] = torch.utils.data.DataLoader(tokenized_dataset['train'], batch_size=10)\ndata['validation'] = torch.utils.data.DataLoader(tokenized_dataset['validation'], batch_size=10)\ntest_data = torch.utils.data.DataLoader(tokenized_dataset['test'], batch_size=10)","metadata":{"id":"4azu_UHZqsIm","execution":{"iopub.status.busy":"2021-12-18T11:52:11.894979Z","iopub.execute_input":"2021-12-18T11:52:11.895617Z","iopub.status.idle":"2021-12-18T11:52:11.901526Z","shell.execute_reply.started":"2021-12-18T11:52:11.895578Z","shell.execute_reply":"2021-12-18T11:52:11.900355Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaForSequenceClassification\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = RobertaForSequenceClassification.from_pretrained(bert_version,num_labels=4).to(device)","metadata":{"id":"otNBYZdJqtnX","outputId":"3ca92ea5-6e1d-49f8-f47b-7d8eae3b2b70","execution":{"iopub.status.busy":"2021-12-18T11:52:11.902570Z","iopub.execute_input":"2021-12-18T11:52:11.902919Z","iopub.status.idle":"2021-12-18T11:52:28.953411Z","shell.execute_reply.started":"2021-12-18T11:52:11.902882Z","shell.execute_reply":"2021-12-18T11:52:28.952425Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Model Training\n\n\n*   The RoBERTa model was trained for 4 epocchs, in batches of size 10.\n*   Due to limited resources, all the epochs could not be completed, hence there exists a keyboard intterupt.\n\n","metadata":{"id":"1-TsOZfVllkh"}},{"cell_type":"code","source":"def train_model(model, optimizer, num_epochs=5, batch_size=4):    \n    train_loss = []\n    curr_loss = {}\n    liveloss = PlotLosses()\n    # for epoch in tqdmn(range(num_epochs)):\n\n    for epoch in range(num_epochs):\n        current_loss = 0\n        # for i, batch in enumerate(tqdmn(train_data)):\n        for i, batch in enumerate(data['train']):\n            model.train()\n            #print(batch)\n            batch = { k: v.to(device) for k, v in batch.items() }\n            outputs = model(**batch)\n            loss = outputs[0]\n            loss.backward()\n\n            current_loss += loss.item()\n            dividor = batch_size * 2 if batch_size < 10 else batch_size\n            if i % dividor == 0 and i > 0:\n                optimizer.step()\n                optimizer.zero_grad()\n                train_loss.append(current_loss / (dividor*batch_size))\n                \n                curr_loss['train loss'] = current_loss/ (dividor*batch_size)\n                liveloss.update(curr_loss)\n                liveloss.send()\n                current_loss = 0\n\n            if i%400 == 0 and i>0:\n                model.eval()\n                validation_loss = 0\n                for i, batch in enumerate(data['validation']):\n                    batch = { k: v.to(device) for k, v in batch.items() }\n                    outputs = model(**batch)\n                    loss = outputs[0]\n                    validation_loss += loss.item()\n                curr_loss['validation loss'] = validation_loss/(i*batch_size)\n                liveloss.update(curr_loss)\n                liveloss.send()\n\n\n        optimizer.step()\n        optimizer.zero_grad()","metadata":{"id":"00W50QHRtAHy","execution":{"iopub.status.busy":"2021-12-18T11:52:28.955732Z","iopub.execute_input":"2021-12-18T11:52:28.956009Z","iopub.status.idle":"2021-12-18T11:52:28.968710Z","shell.execute_reply.started":"2021-12-18T11:52:28.955962Z","shell.execute_reply":"2021-12-18T11:52:28.968001Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.AdamW(params=model.parameters(), lr=1e-5)","metadata":{"id":"crYTt0CYyekv","execution":{"iopub.status.busy":"2021-12-18T11:52:28.969829Z","iopub.execute_input":"2021-12-18T11:52:28.970075Z","iopub.status.idle":"2021-12-18T11:52:28.978419Z","shell.execute_reply.started":"2021-12-18T11:52:28.970041Z","shell.execute_reply":"2021-12-18T11:52:28.977723Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_model(model, optimizer, num_epochs=3,batch_size=10)","metadata":{"id":"iqp5wkz80ZzI","outputId":"04e8f909-a1dc-4806-826a-58bf35f040f1","execution":{"iopub.status.busy":"2021-12-18T11:52:28.979564Z","iopub.execute_input":"2021-12-18T11:52:28.979977Z","iopub.status.idle":"2021-12-18T12:23:33.135639Z","shell.execute_reply.started":"2021-12-18T11:52:28.979942Z","shell.execute_reply":"2021-12-18T12:23:33.134896Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Testing the Model\n\n\n*   test dataset comprising of 1235 samples was used to test the model\n*   the results are described as classifcation report and confusion amtrix below.\n\n","metadata":{"id":"KKdhZANlmHh0"}},{"cell_type":"code","source":"import tqdm\n\n\ntqdmn = tqdm.notebook.tqdm\nmodel = model.eval()\nnum_classes = 4\nconfusion = torch.zeros(num_classes, num_classes)\ny_true, y_pred = [], []\nfor i, batch in enumerate(tqdmn(test_data)):\n    with torch.no_grad():\n        batch = { k: v.to(device) for k, v in batch.items() }\n        outputs = model(**batch)\n        #print(outputs)\n        true_values = batch['labels']\n        pred_values = torch.argmax(outputs[1],dim=1)\n        y_true.extend(true_values)\n        y_pred.extend(pred_values)\n        for true, pred in zip(true_values, pred_values):\n            confusion[true.item()][pred.item()] += 1\n          ","metadata":{"id":"BDHX2Bv6xyNz","outputId":"0cb94177-2ee8-47de-c7bf-fe64a6c61dbb","execution":{"iopub.status.busy":"2021-12-18T12:25:36.197733Z","iopub.execute_input":"2021-12-18T12:25:36.198290Z","iopub.status.idle":"2021-12-18T12:26:00.338693Z","shell.execute_reply.started":"2021-12-18T12:25:36.198254Z","shell.execute_reply":"2021-12-18T12:26:00.337910Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ny_pred = list(map(int, y_pred))\ny_true = list(map(int, y_true))\nfor i in range(num_classes):\n    confusion[i] = confusion[i] / confusion[i].sum()\n  \nprint(metrics.classification_report(y_true, y_pred, digits=3))\n\nlabels = ['false','mixture','true','unproven']\n\nfig, ax = plt.subplots(figsize=(10, 10))\nax.matshow(confusion.numpy())\n\nids = np.arange(len(labels))\nax.set_ylabel('True Labels', fontsize='x-large')\nax.set_xlabel('Pred Labels', fontsize='x-large')\nax.set_xticks(ids)\nax.set_xticklabels(labels)\nax.set_yticks(ids)\nax.set_yticklabels(labels)\n\nfig.tight_layout()\nplt.show()","metadata":{"id":"vu0zZcqv0vXH","outputId":"56c7d84f-6764-46ca-c9e1-bc3cdc8f01e9","execution":{"iopub.status.busy":"2021-12-18T12:26:33.833819Z","iopub.execute_input":"2021-12-18T12:26:33.834323Z","iopub.status.idle":"2021-12-18T12:26:34.177617Z","shell.execute_reply.started":"2021-12-18T12:26:33.834282Z","shell.execute_reply":"2021-12-18T12:26:34.176888Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-12-18T09:50:39.642273Z","iopub.execute_input":"2021-12-18T09:50:39.643007Z","iopub.status.idle":"2021-12-18T09:50:40.497919Z","shell.execute_reply.started":"2021-12-18T09:50:39.642962Z","shell.execute_reply":"2021-12-18T09:50:40.497147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model,\"RoBERT_healthFacts.pt\")","metadata":{"execution":{"iopub.status.busy":"2021-12-18T09:52:39.32712Z","iopub.execute_input":"2021-12-18T09:52:39.327401Z","iopub.status.idle":"2021-12-18T09:52:40.231184Z","shell.execute_reply.started":"2021-12-18T09:52:39.327357Z","shell.execute_reply":"2021-12-18T09:52:40.230422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part - B\n# Using the mebeddings of the model for training an unsupervised learning algortihm (KNN)\n\n\n\n*   In the previous approach, I observed that the majority of the test samples were having tokens > 512 (maximum number of tokens the model can work with) \n*   In the previous approach, the extra tokens were discarded, whcih was loss of crucial information.\n*  So, to overcome that, this apporach is used\n*In this approach, the extra tokens are passed to the model as sections comprising of 512 tokens, to obtain the embeddings\n*once the embeddings was obtained, for a large text the, there were mmore than one embedding vectors.\n*to overcome that, the average of all these vectors was taken to obtain a single embedding.\n* these vectors are then used by the K-Nearest neighbor model to obtain train and predict the outputs of the unknown (test data).  \n*A key point to note here is that the model that has been used for this section, has not been finetuned on the dataset.\n","metadata":{"id":"wvB1ixxdmcBT"}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = RobertaModel.from_pretrained(bert_version).to(device)","metadata":{"id":"rfWWEKDYtpoC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def modelOutputs(batch):\n  with torch.no_grad():\n    outputs = model(batch['input_ids'].to(device), batch['attention_mask'].to(device),output_hidden_states=True)\n    hidden_states = outputs[1]\n    #print(hidden_states)\n    pooled_output = torch.cat(tuple([hidden_states[i] for i in [-4, -3, -2, -1]]), dim=-1)\n    pooled_output = pooled_output[:, 0, :]\n    \n    t = pooled_output[0]\n  return t\n","metadata":{"id":"uxViDM6_wqCl","execution":{"iopub.status.busy":"2021-12-18T11:08:39.240086Z","iopub.execute_input":"2021-12-18T11:08:39.240348Z","iopub.status.idle":"2021-12-18T11:08:39.2473Z","shell.execute_reply.started":"2021-12-18T11:08:39.240317Z","shell.execute_reply":"2021-12-18T11:08:39.246468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encodings = tokenizer(\n          dataset['train'][1]['main_text'], \n          truncation=True, \n          padding='max_length',\n          stride = 50,\n          return_overflowing_tokens=True,\n          return_tensors = \"pt\"\n          )","metadata":{"execution":{"iopub.status.busy":"2021-12-18T09:55:44.17789Z","iopub.execute_input":"2021-12-18T09:55:44.178145Z","iopub.status.idle":"2021-12-18T09:55:44.18916Z","shell.execute_reply.started":"2021-12-18T09:55:44.178115Z","shell.execute_reply":"2021-12-18T09:55:44.188434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encodings","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:42:16.612477Z","iopub.execute_input":"2021-12-18T10:42:16.613233Z","iopub.status.idle":"2021-12-18T10:42:16.627197Z","shell.execute_reply.started":"2021-12-18T10:42:16.613195Z","shell.execute_reply":"2021-12-18T10:42:16.626448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = modelOutputs(encodings)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:03:07.314487Z","iopub.execute_input":"2021-12-18T10:03:07.315265Z","iopub.status.idle":"2021-12-18T10:03:07.334192Z","shell.execute_reply.started":"2021-12-18T10:03:07.315204Z","shell.execute_reply":"2021-12-18T10:03:07.333427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(l[1])","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:31:07.252778Z","iopub.execute_input":"2021-12-18T10:31:07.253558Z","iopub.status.idle":"2021-12-18T10:31:07.258647Z","shell.execute_reply.started":"2021-12-18T10:31:07.253519Z","shell.execute_reply":"2021-12-18T10:31:07.257892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden_states = l[1]","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:31:34.644005Z","iopub.execute_input":"2021-12-18T10:31:34.644659Z","iopub.status.idle":"2021-12-18T10:31:34.649046Z","shell.execute_reply.started":"2021-12-18T10:31:34.644617Z","shell.execute_reply":"2021-12-18T10:31:34.648369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:54:37.621035Z","iopub.execute_input":"2021-12-18T10:54:37.621292Z","iopub.status.idle":"2021-12-18T10:54:37.632005Z","shell.execute_reply.started":"2021-12-18T10:54:37.621261Z","shell.execute_reply":"2021-12-18T10:54:37.631278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pooled_output = torch.cat(tuple([hidden_states[i] for i in [-4, -3, -2, -1]]), dim=-1)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:31:37.963761Z","iopub.execute_input":"2021-12-18T10:31:37.964317Z","iopub.status.idle":"2021-12-18T10:31:37.968959Z","shell.execute_reply.started":"2021-12-18T10:31:37.964275Z","shell.execute_reply":"2021-12-18T10:31:37.968081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pooled_output[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:32:15.815126Z","iopub.execute_input":"2021-12-18T10:32:15.815407Z","iopub.status.idle":"2021-12-18T10:32:15.824943Z","shell.execute_reply.started":"2021-12-18T10:32:15.815359Z","shell.execute_reply":"2021-12-18T10:32:15.824183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pooled_output = pooled_output[:, 0, :]","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:32:57.755185Z","iopub.execute_input":"2021-12-18T10:32:57.755523Z","iopub.status.idle":"2021-12-18T10:32:57.761459Z","shell.execute_reply.started":"2021-12-18T10:32:57.755481Z","shell.execute_reply":"2021-12-18T10:32:57.759037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pooled_output","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:33:10.832613Z","iopub.execute_input":"2021-12-18T10:33:10.832887Z","iopub.status.idle":"2021-12-18T10:33:10.841572Z","shell.execute_reply.started":"2021-12-18T10:33:10.832855Z","shell.execute_reply":"2021-12-18T10:33:10.840298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.tensor([2]*100)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:47:05.204259Z","iopub.execute_input":"2021-12-18T10:47:05.204836Z","iopub.status.idle":"2021-12-18T10:47:05.211321Z","shell.execute_reply.started":"2021-12-18T10:47:05.204788Z","shell.execute_reply":"2021-12-18T10:47:05.210632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len_of_tokens_in_BERT = 510\nATTN_MASK = torch.tensor([[1]*512])\nSTART_TOKEN = torch.tensor([0]) #start token for RoBERTa,\nEND_TOKEN = torch.tensor([2]) #end token for RoBERTa,\n\ndef getEncodedLargeSnetences(num_tokens,tokens, encd):\n  \"\"\"this function encoded the large senetences\"\"\"\n  \"\"\"\n  Inputs -  number of extra tokens, extra tokens, and the first 512 encoded tokens.\n  Outputs - encoded string by BERT model for the senetence\n  functionality - Get the broken down embeddings of large text, take the average of these embeddings\n  \"\"\"\n  i = 0\n  n = 1 #used to calculate the toal number of sets for the large text.\n  encd.pop('overflowing_tokens')\n  encd.pop('num_truncated_tokens')\n  batch = { k: v.to(device) for k, v in encd.items() }\n\n  temp = modelOutputs(batch)\n\n  encodings = None\n \n  while i+max_len_of_tokens_in_BERT < num_tokens:\n    attn_mask = None\n    n+=1\n\n    if i+max_len_of_tokens_in_BERT < num_tokens:\n      #inital set of extra tokens\n      #lasredy has the start token\n      reduced_text = torch.cat((START_TOKEN,tokens[i:i+max_len_of_tokens_in_BERT],END_TOKEN),0)\n      attn_mask = ATTN_MASK\n          \n    else:\n      #final set of extra tokens\n      #already has the end token\n      j = num_tokens - i\n      reduced_text = torch.cat((START_TOKEN,tokens[i:num_tokens],END_TOKEN,torch.tensor([1]*(512-j))),0)\n      attn_mask = torch.cat((torch.tensor([1]*j),torch.tensor([0]*(512-j),0)))\n      attn_mask = attn_mask.unsqueeze(0)\n\n    reduced_text = reduced_text.unsqueeze(0)\n\n    encodings = {'input_ids': reduced_text,'attention_mask': attn_mask} #input tyoe for BERT model created\n    inputs = transformers.tokenization_utils_base.BatchEncoding(encodings)\n    batch = { k: v.to(device) for k, v in inputs.items() }\n    #print(batch)\n    outputs = modelOutputs(batch)\n    \n    if temp == None:\n      temp = outputs\n    else:\n      temp.add(outputs)\n\n    i+=(max_len_of_tokens_in_BERT-50)#having 50 tokens from the previous set, overlapped\n\n  temp = torch.div(temp, n) #avergae of all the vecors associated with the large text\n  return temp  ","metadata":{"id":"2Bq2M07Q7MfF","execution":{"iopub.status.busy":"2021-12-18T11:10:36.424796Z","iopub.execute_input":"2021-12-18T11:10:36.425122Z","iopub.status.idle":"2021-12-18T11:10:36.438835Z","shell.execute_reply.started":"2021-12-18T11:10:36.425089Z","shell.execute_reply":"2021-12-18T11:10:36.437839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getEmbeddings(dataSplit, encoded_array=None):\n  \"\"\"this function converts the text into embeddings\"\"\"\n  \"\"\"\n  Inputs -  the dataset type (train, test,valiadtion)\n  Outputs - an array of encoded string by BERT model.\n  \"\"\"\n  for i in dataset[dataSplit]:\n    tmp = None\n    encodings = tokenizer(\n          i['main_text'], \n          truncation=True, \n          padding='max_length',\n          stride = 50,\n          return_overflowing_tokens=True,\n          return_tensors = \"pt\"\n          ) #tokenier is applied\n    if 'overflowing_tokens' in encodings.keys():\n      if len(encodings['overflowing_tokens']) > 0:\n        #if there are overflow tokens, then the text is large and needs to handled\n        remianing_token_size = encodings['num_truncated_tokens'][0]\n        remaining_tokens = encodings['overflowing_tokens'][0]\n        tmp = getEncodedLargeSnetences(remianing_token_size,remaining_tokens,encodings) #obtain the single embedding vector for the large text\n      \n\n    else:\n      #case where the text is small, <512 tokens.\n      encodings.pop('overflowing_tokens')\n      encodings.pop('num_truncated_tokens')\n      batchn = { k: v.to(device) for k, v in encodings.items() }\n      tmp = modelOutputs(batchn)\n      \n\n\n    if encoded_array == None:\n      encoded_array = tmp\n    else:\n      encoded_array = torch.vstack((encoded_array,tmp))\n  return encoded_array  ","metadata":{"id":"rbDYJ6qT78eJ","execution":{"iopub.status.busy":"2021-12-18T11:10:39.871933Z","iopub.execute_input":"2021-12-18T11:10:39.872487Z","iopub.status.idle":"2021-12-18T11:10:39.880093Z","shell.execute_reply.started":"2021-12-18T11:10:39.872441Z","shell.execute_reply":"2021-12-18T11:10:39.879449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for obtaining training data\ndef getLabels(dataSplit,labelArr):\n  \"\"\"this function obtains the labels for samples\"\"\"\n  \"\"\"\n  Inputs -  the dataset type (train, test,valiadtion)\n  Outputs - an array of labels.\n  \"\"\"\n  for i in dataset[dataSplit]:\n    if i['label'] == 0:\n      labelArr.append('False')\n    elif i['label'] == 1:\n      labelArr.append('Mixture')\n    elif i['label'] == 2:\n      labelArr.append('True')\n    else:\n      labelArr.append('Unproven')\n\n  return labelArr\n\n\ndef obtainSplitWiseEmbeddings(dataSplit,mergeTrainValidation=False):\n  \"\"\"this function obtains the labels and embeddings vectors for samples\"\"\"\n  \"\"\"\n  Inputs -  the dataset type (train, test,valiadtion)\n  Outputs - an array of labels and embedding vector for that set.\n  \"\"\"\n  encoded_array, labelArr = None, []\n  if dataSplit == \"train\":\n    if mergeTrainValidation:\n\n      encoded_array = getEmbeddings('train',encoded_array)\n      encoded_array = getEmbeddings('validation', encoded_array)\n      labelArr = getLabels('train',[])\n      labelArr = getLabels('validation',labelArr)\n    else:\n      encoded_array = getEmbeddings('train',encoded_array)\n      labelArr = getLabels('train',[])\n\n  else:\n    encoded_array = getEmbeddings(dataSplit,encoded_array)\n    labelArr = getLabels(dataSplit,[])\n      \n  return [encoded_array,labelArr]","metadata":{"id":"h4q77H8JPq8x","execution":{"iopub.status.busy":"2021-12-18T11:10:46.939908Z","iopub.execute_input":"2021-12-18T11:10:46.940168Z","iopub.status.idle":"2021-12-18T11:10:46.950009Z","shell.execute_reply.started":"2021-12-18T11:10:46.940138Z","shell.execute_reply":"2021-12-18T11:10:46.94893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, '/kaggle/working/model.pt')","metadata":{"execution":{"iopub.status.busy":"2021-12-18T10:59:31.718666Z","iopub.execute_input":"2021-12-18T10:59:31.718924Z","iopub.status.idle":"2021-12-18T10:59:32.592563Z","shell.execute_reply.started":"2021-12-18T10:59:31.718895Z","shell.execute_reply":"2021-12-18T10:59:32.591783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#saving the embeddings for train\n\n\ntrainEmbeddings, trainLabels = obtainSplitWiseEmbeddings('train',False)\n#torch.save(trainEmbeddings, trainEncodeFileName)\n","metadata":{"id":"CDBjlvWM3rf-","execution":{"iopub.status.busy":"2021-12-18T11:11:11.990763Z","iopub.execute_input":"2021-12-18T11:11:11.991021Z","iopub.status.idle":"2021-12-18T11:17:00.3727Z","shell.execute_reply.started":"2021-12-18T11:11:11.990991Z","shell.execute_reply":"2021-12-18T11:17:00.371885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(trainEmbeddings)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:19:29.377614Z","iopub.execute_input":"2021-12-18T11:19:29.377891Z","iopub.status.idle":"2021-12-18T11:19:29.388654Z","shell.execute_reply.started":"2021-12-18T11:19:29.377863Z","shell.execute_reply":"2021-12-18T11:19:29.387725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#saving the embeddings for the test\n\n\ntestEmbeddings, testLabels = obtainSplitWiseEmbeddings('test',False)\n#torch.save(testEmbeddings, testEncodeFileName)","metadata":{"id":"tPSC6bR_Zoeq","execution":{"iopub.status.busy":"2021-12-18T11:17:50.238553Z","iopub.execute_input":"2021-12-18T11:17:50.238822Z","iopub.status.idle":"2021-12-18T11:18:33.979153Z","shell.execute_reply.started":"2021-12-18T11:17:50.238791Z","shell.execute_reply":"2021-12-18T11:18:33.978433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainEncodeFileName = '/kaggle/working/encoded.pt'\ntrainLabelFileName = '/kaggle/working/labels.pt'\ntestEncodeFileName = '/kaggle/working/encodedTest.pt'\ntestLabelFileName = '/kaggle/working/testLabels.pt'","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:37:02.540211Z","iopub.execute_input":"2021-12-18T11:37:02.540474Z","iopub.status.idle":"2021-12-18T11:37:02.544336Z","shell.execute_reply.started":"2021-12-18T11:37:02.540444Z","shell.execute_reply":"2021-12-18T11:37:02.543403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(trainEmbeddings,trainEncodeFileName)\ntorch.save(testEmbeddings,testEncodeFileName)\n\n\n#torch.save(torch.tensor(trainLabels),trainLabelFileName)\n#torch.save(torch.tensor(testLabels),testLabelFileName)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:23:16.91824Z","iopub.execute_input":"2021-12-18T11:23:16.919042Z","iopub.status.idle":"2021-12-18T11:23:17.354821Z","shell.execute_reply.started":"2021-12-18T11:23:16.918992Z","shell.execute_reply":"2021-12-18T11:23:17.35399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open(trainLabelFileName, 'wb') as file:\n      \n    # A new file will be created\n    pickle.dump(trainLabels,file)\n    #pickle.dump(myvar, file)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:36:07.20312Z","iopub.execute_input":"2021-12-18T11:36:07.203815Z","iopub.status.idle":"2021-12-18T11:36:07.209457Z","shell.execute_reply.started":"2021-12-18T11:36:07.203772Z","shell.execute_reply":"2021-12-18T11:36:07.208521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(testLabelFileName, 'wb') as file:\n      \n    # A new file will be created\n    pickle.dump(testLabels,file)\n    #pickle.dump(myvar, file)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:36:31.328822Z","iopub.execute_input":"2021-12-18T11:36:31.329114Z","iopub.status.idle":"2021-12-18T11:36:31.333835Z","shell.execute_reply.started":"2021-12-18T11:36:31.329084Z","shell.execute_reply":"2021-12-18T11:36:31.333151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#utilitied to draw the scatter plot\nfrom sklearn.manifold import TSNE\ndef getColor(val):\n  if val == \"False\":\n    return \"red\"\n  elif val == \"Mixture\":\n    return \"blue\"\n  elif val == \"True\":\n    return \"green\"\n  else:\n    return \"black\"\n\n\n#TSNE dimensionality reductio for reducing the dimension of \n#embeddings to 2, in order to plot them.\n\n\n#tsne_cuda = TSNE(n_components=2, verbose=0)\n#newArr = tsne_cuda.fit_transform()\nnewArr = TSNE(n_components=2, learning_rate='auto',init='random').fit_transform(trainEmbeddings.cpu().numpy())\ndf = pd.DataFrame(newArr, columns=[\"x\", \"y\"])\ndf[\"val\"] = pd.Series(trainLabels).apply(lambda x: getColor(x))","metadata":{"id":"-wraNV4q-7Qp","execution":{"iopub.status.busy":"2021-12-18T11:25:57.341177Z","iopub.execute_input":"2021-12-18T11:25:57.341458Z","iopub.status.idle":"2021-12-18T11:30:53.687826Z","shell.execute_reply.started":"2021-12-18T11:25:57.341426Z","shell.execute_reply":"2021-12-18T11:30:53.686581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scatterPlot(df):\n  \"\"\"this function draws the scatter plot for given data\"\"\"\n \n  plt.figure(figsize=(16,10))\n  palette = sns.hls_palette(4, l=.6, s=.9)\n  sns.scatterplot(\n      x= df['x'], y= df['y'], c=df['val'],\n      palette= palette,\n      legend=\"full\",\n      hue=labelArr,\n      alpha=0.3,\n  )\n  plt.show()","metadata":{"id":"AzIglcFOCxwE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scatter plot\n\n\n\n*   As we can see from the scatter plot below, the data is spread across.\n*   Looking at this, we can form multiple clusters\n* but since we have only 4 classes, we would need only four clusters.\n* this might result in improper classification for classes that are spread across or are mixed together, like the false and mixture.\n* to overcome this, i use the KNN algortihm.\n\n","metadata":{"id":"a7_JWOrKxwQS"}},{"cell_type":"code","source":"scatterPlot(df)","metadata":{"id":"MbVpcdPRDRi8","outputId":"0939bb20-5024-4752-fde6-6aff351dc490"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ndef getClassifier(k):\n  neigh = KNeighborsClassifier(n_neighbors=k, weights='distance',algorithm='kd_tree' )\n  neigh.fit(encoded_array.cpu().numpy(), labelArr)\n  return neigh","metadata":{"id":"_meKoRBsaHC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getPreds(neigh):\n  predictions = []\n  for i in testEmbeddings.cpu().numpy():\n    predictions.append(neigh.predict(i.reshape(1,-1)))\n\n  return predictions","metadata":{"id":"Tsi9VsFebV6C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#k=5\nneig = getClassifier(5)\npredictions = getpreds(neig)\nprint(classification_report(testLabels, predictions, digits=3))\n","metadata":{"id":"iLC_Y74xcbdP","outputId":"d1bfa175-2f16-4a06-9e0e-f2d178d5252f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#k=10\nneig = getClassifier(10)\npredictions = getpreds(neig)\nprint(classification_report(testLabels, predictions, digits=3))","metadata":{"id":"pblrbzVXeAxL","outputId":"1bb8415e-0738-4a52-85e8-a08816e520ac"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Improvements\n\n\n\n*   As seen above, wven without the finetuning of the dataset, approach B had comparable results\n*   This can be further imprvoed by finetuning the model on the dataset\n*Furthermore, BERT based model trained on the medical datasets like BioBERT, and clinical data like the clinicalBERT can be used for this task.\n*unfortunately, it could not be implemented here because of limited resources availability\n\n","metadata":{"id":"zfAQOJPV01Dj"}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-12-18T12:42:07.580543Z","iopub.execute_input":"2021-12-18T12:42:07.580898Z","iopub.status.idle":"2021-12-18T12:42:08.278570Z","shell.execute_reply.started":"2021-12-18T12:42:07.580867Z","shell.execute_reply":"2021-12-18T12:42:08.277615Z"},"trusted":true},"execution_count":18,"outputs":[]}]}